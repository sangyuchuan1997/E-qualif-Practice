{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oUFlC0MsUyyzsId4tyuZZO75eZqtmRTp","timestamp":1645499265236}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 修了課題DEMO②　MNIST"],"metadata":{"id":"MpWPASHUaE8I"}},{"cell_type":"markdown","source":["##はじめに\n","MNISTとは手書き数字の画像データセットのことです。\n","\n","今回はそのMNISTから画像を学習して数字を精度が高く識別できるモデルを作成してみましょう。\n","\n","今回はDEMOとして、手書き認識モデルとして一般的なDeepConvNetを取り上げました。\n","\n","DeepConvNetのネットワーク構成は\n","    \n","    conv - relu - conv- relu - pool -\n","    conv - relu - conv- relu - pool -\n","    conv - relu - conv- relu - pool -\n","    affine - relu - dropout - affine - dropout - SoftmaxWithLoss\n","\n","上記のように計２１層となっています。"],"metadata":{"id":"YVDhKG_raLJq"}},{"cell_type":"markdown","source":["##作成までの流れ\n","大まかな流れとして\n","1. データのダウンロードと成形\n","\n","   データダウンロードしてそのまま活用するのは困難です。\n","   モデルが学習できるよう、カテゴリ変数に置き換えたり、\n","   欠損値を補完したりなど、成形する必要があります。\n","2. モデルの構築\n","\n","   学習を行うモデルのアルゴリズムを理解して、コードを作成します。\n","\n","3. 学習と結果\n","\n","   学習を行い結果を確認してみます。\n","   精度が目標まで達したら、提出用のデータを作成します。"],"metadata":{"id":"QVOc238jnrhX"}},{"cell_type":"markdown","metadata":{"id":"f52GjhwuKoj4"},"source":["## 必要なライブラリーのインポート"]},{"cell_type":"code","metadata":{"id":"3jW0yZ9VMKFS"},"source":["# GPUを利用する場合\n","import cupy as np\n","# CPUを利用する場合\n","# import numpy as np\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#1.データのダウンロードと成形"],"metadata":{"id":"OBy0sscFp2eZ"}},{"cell_type":"markdown","metadata":{"id":"Vuqdr3mXMJ5K"},"source":["## データのダウンロード"]},{"cell_type":"markdown","source":["今回はDeepConvNetの入力形式に合わせるため、\n","\n","正規化を行った後にそれぞれの画像の次元に(28,28)の次元を追加しています。"],"metadata":{"id":"A-YqVpsAeUZd"}},{"cell_type":"code","metadata":{"id":"Mwz8AzSDMH_X"},"source":["# MNISTデータのロードと訓練データ、テストデータの分割を行う\n","def load_mnist():\n","    mnist = fetch_openml('MNIST_784', version=1, data_home=\"MNIST_data/\")\n","    mnist_X, mnist_y = shuffle(mnist.data[:60000].astype('float32'),\n","                               mnist.target[:60000].astype('int32'), random_state=42)\n","\n","    # 正規化を行う\n","    mnist_X = mnist_X / 255.0\n","    mnist_X = np.array(mnist_X)\n","    train_images = []\n","    #次元を追加する\n","    for i in range(mnist_X.shape[0]):\n","        train_images.append(np.expand_dims(mnist_X[i].reshape(28, 28), axis=0))\n","    mnist_X = np.array(train_images)\n","\n","    # one-hot表現に変換する\n","    mnist_y = np.eye(10)[np.array(mnist_y)]\n","\n","    return train_test_split(mnist_X, mnist_y,\n","                test_size=0.2,\n","                random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAqC-76JMFyr"},"source":["# それぞれのデータの形式を出力する\n","x_train, x_val, y_train, y_val = load_mnist()\n","print('学習データ ', x_train.shape)\n","print('学習データの正解ラベル ', y_train.shape)\n","print('検証データ ', x_val.shape)\n","print('検証データの正解ラベル ', y_val.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.モデルの構築"],"metadata":{"id":"oaWNFvmdqJiP"}},{"cell_type":"markdown","metadata":{"id":"R148IpZuME8G"},"source":["## 必要な基本関数の定義"]},{"cell_type":"code","metadata":{"id":"mAWnjpXULE7p"},"source":["# シグモイド関数\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# シグモイド関数の微分\n","def sigmoid_grad(x):\n","    return (1.0 - sigmoid(x)) * sigmoid(x)\n","\n","# relu関数\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","# relu関数の微分\n","def relu_grad(x):\n","    grad = np.zeros_like(x)\n","    grad[x>=0] = 1\n","    return grad\n","\n","# ソフトマックス関数\n","def softmax(x):\n","    x = x - np.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\n","    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n","\n","# 交差エントロピー誤差\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","# ソフトマックス損失\n","def softmax_loss(X, t):\n","    y = softmax(X)\n","    return cross_entropy_error(y, t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57jSEh9vKn_2"},"source":["# Relu関数のクラス\n","class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","\n","# シグモイド関数のクラス\n","class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = sigmoid(x)\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx\n","\n","# アフィン層(全結合層)のクラス\n","class Affine:\n","    def __init__(self, W, b):\n","        self.W =W\n","        self.b = b\n","\n","        self.x = None\n","        self.original_x_shape = None\n","        # 重み・バイアスパラメータの微分\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        # テンソル対応\n","        self.original_x_shape = x.shape\n","        x = x.reshape(x.shape[0], -1)\n","        self.x = x\n","\n","        out = np.dot(self.x, self.W) + self.b\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","\n","        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n","        return dx\n","\n","# ソフトマックス損失層のクラス\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None\n","        self.y = None # softmaxの出力\n","        self.t = None # 教師データ\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","        self.loss = cross_entropy_error(self.y, self.t)\n","\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n","            dx = (self.y - self.t) / batch_size\n","        else:\n","            dx = self.y.copy()\n","            dx[np.arange(batch_size), self.t] -= 1\n","            dx = dx / batch_size\n","\n","        return dx\n","\n","# 確率的勾配降下法（Stochastic Gradient Descent)のクラス\n","class SGD:\n","    def __init__(self, lr=0.01):\n","        # 学習率を初期化する（0.01にする）。\n","        self.lr = lr\n","\n","    def update(self, params, grads):\n","        # パラメータの更新を行うメソッド\n","        # params: 更新されるパラメータ（例：ニューラルネットワークの重みやバイアス）\n","        # grads: 各パラメータに関する勾配（損失関数の勾配）\n","        for key in params.keys():\n","            # 各パラメータに対して、勾配に基づいて更新を行う。\n","            params[key] -= self.lr * grads[key]\n","\n","# ドロップアウト層の関数\n","class Dropout:\n","    def __init__(self, dropout_ratio=0.5):\n","        # ドロップアウトの割合を設定する（デフォルトは0.5）\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","\n","    def forward(self, x, train_flg=True):\n","        if train_flg:\n","            # 訓練時は、ランダムにノードをドロップアウトする。\n","            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n","            # マスクを適用して出力を返す。\n","            return x * self.mask\n","        else:\n","            # 評価時は、ドロップアウトは適用せず、\n","            # 各ノードの出力をドロップアウト比率でスケーリングする。\n","            return x * (1.0 - self.dropout_ratio)\n","\n","    def backward(self, dout):\n","        # 逆伝播時にもマスクを適用する。\n","        # これにより、前方伝播時にドロップアウトされたノードは、\n","        # 逆伝播でも影響を与えない。\n","        return dout * self.mask\n","\n","# バッチノーマライゼーションのクラス\n","class BatchNormalization:\n","    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n","        self.gamma = gamma\n","        self.beta = beta\n","        self.momentum = momentum\n","        self.input_shape = None # Conv層の場合は4次元、全結合層の場合は2次元\n","\n","        # テスト時に使用する平均と分散\n","        self.running_mean = running_mean\n","        self.running_var = running_var\n","\n","        # backward時に使用する中間データ\n","        self.batch_size = None\n","        self.xc = None\n","        self.std = None\n","        self.dgamma = None\n","        self.dbeta = None\n","\n","    def forward(self, x, train_flg=True):\n","        self.input_shape = x.shape\n","        if x.ndim != 2:\n","            N, C, H, W = x.shape\n","            x = x.reshape(N, -1)\n","\n","        out = self.__forward(x, train_flg)\n","\n","        return out.reshape(*self.input_shape)\n","\n","    def __forward(self, x, train_flg):\n","        if self.running_mean is None:\n","            N, D = x.shape\n","            self.running_mean = np.zeros(D)\n","            self.running_var = np.zeros(D)\n","\n","        if train_flg:\n","            mu = x.mean(axis=0)\n","            xc = x - mu\n","            var = np.mean(xc**2, axis=0)\n","            std = np.sqrt(var + 10e-7)\n","            xn = xc / std\n","\n","            self.batch_size = x.shape[0]\n","            self.xc = xc\n","            self.xn = xn\n","            self.std = std\n","            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n","            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n","        else:\n","            xc = x - self.running_mean\n","            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n","\n","        out = self.gamma * xn + self.beta\n","        return out\n","\n","    def backward(self, dout):\n","        if dout.ndim != 2:\n","            N, C, H, W = dout.shape\n","            dout = dout.reshape(N, -1)\n","\n","        dx = self.__backward(dout)\n","\n","        dx = dx.reshape(*self.input_shape)\n","        return dx\n","\n","    def __backward(self, dout):\n","        dbeta = dout.sum(axis=0)\n","        dgamma = np.sum(self.xn * dout, axis=0)\n","        dxn = self.gamma * dout\n","        dxc = dxn / self.std\n","        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n","        dvar = 0.5 * dstd / self.std\n","        dxc += (2.0 / self.batch_size) * self.xc * dvar\n","        dmu = np.sum(dxc, axis=0)\n","        dx = dxc - dmu / self.batch_size\n","\n","        self.dgamma = dgamma\n","        self.dbeta = dbeta\n","\n","        return dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Cxo2ZIAThhA"},"source":["## im2col関数とcol2im関数の実装"]},{"cell_type":"code","metadata":{"id":"Uph54-4hK8Sb"},"source":["# im2col関数\n","def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n","    N, C, H, W = input_data.shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","\n","    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n","    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n","\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n","\n","    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n","    return col\n","\n","# col2im関数\n","def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n","    N, C, H, W = input_shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n","\n","    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n","\n","    return img[:, :, pad:H + pad, pad:W + pad]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMIg-49rT5bY"},"source":["## Convlution層とプーリング層の実装"]},{"cell_type":"code","metadata":{"id":"Tgt79o9lT5R-"},"source":["# Convlution層のクラス\n","class Convolution:\n","    def __init__(self, W, b, stride=1, pad=0):\n","        self.W = W\n","        self.b = b\n","        self.stride = stride\n","        self.pad = pad\n","\n","        # 中間データ（backward時に使用）\n","        self.x = None\n","        self.col = None\n","        self.col_W = None\n","\n","        # 重み・バイアスパラメータの勾配\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        FN, C, FH, FW = self.W.shape\n","        N, C, H, W = x.shape\n","        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n","        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n","\n","        col = im2col(x, FH, FW, self.stride, self.pad)\n","        col_W = self.W.reshape(FN, -1).T\n","\n","        out = np.dot(col, col_W) + self.b\n","        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.col = col\n","        self.col_W = col_W\n","\n","        return out\n","\n","    def backward(self, dout):\n","        FN, C, FH, FW = self.W.shape\n","        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n","\n","        self.db = np.sum(dout, axis=0)\n","        self.dW = np.dot(self.col.T, dout)\n","        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n","\n","        dcol = np.dot(dout, self.col_W.T)\n","        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n","\n","        return dx\n","\n","# プーリング層のクラス\n","class Pooling:\n","    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n","        self.pool_h = pool_h\n","        self.pool_w = pool_w\n","        self.stride = stride\n","        self.pad = pad\n","\n","        self.x = None\n","        self.arg_max = None\n","\n","    def forward(self, x):\n","        N, C, H, W = x.shape\n","        out_h = int(1 + (H - self.pool_h) / self.stride)\n","        out_w = int(1 + (W - self.pool_w) / self.stride)\n","\n","        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n","        col = col.reshape(-1, self.pool_h*self.pool_w)\n","\n","        arg_max = np.argmax(col, axis=1)\n","        out = np.max(col, axis=1)\n","        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.arg_max = arg_max\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout = dout.transpose(0, 2, 3, 1)\n","\n","        pool_size = self.pool_h * self.pool_w\n","        dmax = np.zeros((dout.size, pool_size))\n","        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n","        dmax = dmax.reshape(dout.shape + (pool_size,))\n","\n","        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n","        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n","\n","        return dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzyeEXRrKmsZ"},"source":["## CNNの実装"]},{"cell_type":"code","metadata":{"id":"DGYcD3HIKdP5"},"source":["class DeepConvNet:\n","    def __init__(self, input_dim=(1, 28, 28),\n","                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n","                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n","                 hidden_size=50, output_size=10):\n","        # 重みの初期化===========\n","        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか\n","        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n","        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n","\n","        self.params = {}\n","        pre_channel_num = input_dim[0]\n","        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n","            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n","            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n","            pre_channel_num = conv_param['filter_num']\n","        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n","        self.params['b7'] = np.zeros(hidden_size)\n","        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n","        self.params['b8'] = np.zeros(output_size)\n","\n","        # レイヤの生成===========\n","        self.layers = []\n","        self.layers.append(Convolution(self.params['W1'], self.params['b1'],\n","                           conv_param_1['stride'], conv_param_1['pad']))\n","        self.layers.append(Relu())\n","        self.layers.append(Convolution(self.params['W2'], self.params['b2'],\n","                           conv_param_2['stride'], conv_param_2['pad']))\n","        self.layers.append(Relu())\n","        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n","        self.layers.append(Convolution(self.params['W3'], self.params['b3'],\n","                           conv_param_3['stride'], conv_param_3['pad']))\n","        self.layers.append(Relu())\n","        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n","                           conv_param_4['stride'], conv_param_4['pad']))\n","        self.layers.append(Relu())\n","        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n","        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n","                           conv_param_5['stride'], conv_param_5['pad']))\n","        self.layers.append(Relu())\n","        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n","                           conv_param_6['stride'], conv_param_6['pad']))\n","        self.layers.append(Relu())\n","        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n","        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n","        self.layers.append(Relu())\n","        self.layers.append(Dropout(0.5))\n","        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n","        self.layers.append(Dropout(0.5))\n","\n","        self.last_layer = SoftmaxWithLoss()\n","\n","    def predict(self, x, train_flg=False):\n","        # ネットワークを通して入力データxを順伝播させる。\n","        # train_flgがFalseの場合、モデルは評価モードで動作する。\n","\n","        for layer in self.layers:\n","            # ネットワークの各層を順に処理する。\n","            if isinstance(layer, Dropout):\n","                # 層がドロップアウト層の場合、\n","                # train_flgに応じてドロップアウトを適用するか決定する。\n","                x = layer.forward(x, train_flg)\n","            else:\n","                # ドロップアウト層でない場合、通常の順伝播を行う。\n","                x = layer.forward(x)\n","        return x\n","\n","    def loss(self, x, t):\n","        # ネットワークを通じて入力データxを順伝播させ、予測値yを得る。\n","        y = self.predict(x, train_flg=True)\n","\n","        # 最終層（通常は損失層）を使用して、\n","        # 予測値yと真のターゲットtとの間の損失を計算する。\n","        return self.last_layer.forward(y, t)\n","\n","    def accuracy(self, x, t, batch_size=100):\n","        # ターゲットデータtがone-hotエンコーディングされている場合、\n","        # 最大値のインデックス（クラスラベル）に変換する。\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","\n","        acc = 0.0\n","\n","         # バッチ処理を行い、各バッチに対する精度を計算\n","        for i in range(int(x.shape[0] / batch_size)):\n","            # バッチを抽出\n","            tx = x[i*batch_size:(i+1)*batch_size]\n","            tt = t[i*batch_size:(i+1)*batch_size]\n","\n","            # 予測を計算（評価モードなので、train_flgはFalse）\n","            y = self.predict(tx, train_flg=False)\n","            # 予測結果の最大値のインデックスを取得\n","            y = np.argmax(y, axis=1)\n","\n","            # 正解数を加算\n","            acc += np.sum(y == tt)\n","\n","        # 全データに対する精度を計算して返す\n","        return acc / x.shape[0]\n","\n","    def gradient(self, x, t):\n","        # forward\n","        self.loss(x, t)\n","\n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","\n","        tmp_layers = self.layers.copy()\n","        tmp_layers.reverse()\n","        for layer in tmp_layers:\n","            dout = layer.backward(dout)\n","\n","        # 設定\n","        grads = {}\n","        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n","            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n","            grads['b' + str(i+1)] = self.layers[layer_idx].db\n","\n","        return grads\n","\n","    def save_params(self, file_name=\"params.pkl\"):\n","        params = {}\n","        for key, val in self.params.items():\n","            params[key] = val\n","        with open(file_name, 'wb') as f:\n","            pickle.dump(params, f)\n","\n","    def load_params(self, file_name=\"params.pkl\"):\n","        with open(file_name, 'rb') as f:\n","            params = pickle.load(f)\n","        for key, val in params.items():\n","            self.params[key] = val\n","\n","        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n","            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n","            self.layers[layer_idx].b = self.params['b' + str(i+1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2Kyy4X8UDCz"},"source":["## 学習用のクラス"]},{"cell_type":"code","metadata":{"id":"l9wsPRBFKwgi"},"source":["# ニューラルネットの訓練を行うクラス\n","class Trainer:\n","\n","    def __init__(self, network, x_train, t_train, x_val, t_val,\n","                 epochs=20, mini_batch_size=100,\n","                 optimizer='SGD', optimizer_param={'lr':0.01},\n","                 evaluate_sample_num_per_epoch=None, verbose=True):\n","        self.network = network\n","        self.verbose = verbose\n","        self.x_train = x_train\n","        self.t_train = t_train\n","        self.x_val = x_val\n","        self.t_val = t_val\n","        self.epochs = epochs\n","        self.batch_size = mini_batch_size\n","        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n","\n","        # optimizer\n","        optimizer_class_dict = {'sgd':SGD}\n","        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n","\n","        self.train_size = x_train.shape[0]\n","        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n","        self.max_iter = int(epochs * self.iter_per_epoch)\n","        self.current_iter = 0\n","        self.current_epoch = 0\n","\n","        self.train_loss_list = []\n","        self.train_acc_list = []\n","        self.val_acc_list = []\n","\n","    def train_step(self):\n","        batch_mask = np.random.choice(self.train_size, self.batch_size)\n","        x_batch = self.x_train[batch_mask]\n","        t_batch = self.t_train[batch_mask]\n","\n","        grads = self.network.gradient(x_batch, t_batch)\n","        self.optimizer.update(self.network.params, grads)\n","\n","        loss = self.network.loss(x_batch, t_batch)\n","        self.train_loss_list.append(loss)\n","        if self.verbose: print(\"train loss:\" + str(loss))\n","\n","        if self.current_iter % self.iter_per_epoch == 0:\n","            self.current_epoch += 1\n","\n","            x_train_sample, t_train_sample = self.x_train, self.t_train\n","            x_val_sample, t_val_sample = self.x_val, self.t_val\n","            if not self.evaluate_sample_num_per_epoch is None:\n","                t = self.evaluate_sample_num_per_epoch\n","                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n","                x_val_sample, t_val_sample = self.x_val[:t], self.t_val[:t]\n","\n","            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n","            val_acc = self.network.accuracy(x_val_sample, t_val_sample)\n","            self.train_acc_list.append(train_acc)\n","            self.val_acc_list.append(val_acc)\n","\n","            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", val acc:\" + str(val_acc) + \" ===\")\n","        self.current_iter += 1\n","\n","    def train(self):\n","        for i in range(self.max_iter):\n","            self.train_step()\n","            if i % self.iter_per_epoch == 0:\n","              print(int(i//self.iter_per_epoch)+1, 'epoch')\n","              val_acc = self.network.accuracy(self.x_val, self.t_val)\n","              print(\"val acc:\" + str(val_acc))\n","\n","        val_acc = self.network.accuracy(self.x_val, self.t_val)\n","        print(\"=============== 最終 val Accuracy ===============\")\n","        print(\"val acc:\" + str(val_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3.学習から提出まで"],"metadata":{"id":"E5vravpqnUsd"}},{"cell_type":"markdown","metadata":{"id":"sQPEuAJsKbwL"},"source":["## 学習"]},{"cell_type":"code","metadata":{"id":"5YMTI4jyKQn5"},"source":["# DeepConvNetネットワークモデルのインスタンスを作成する。\n","network = DeepConvNet()\n","\n","# Trainerクラスのインスタンスを作成し、ネットワークの訓練を行う。\n","trainer = Trainer(network, x_train, y_train, x_val, y_val,\n","                  epochs=10,            # 学習を行うエポック数\n","                  mini_batch_size=200,  # ミニバッチのサイズ\n","                  optimizer='SGD',      # 最適化アルゴリズムとして確率的勾配降下法（SGD）を使用\n","                  optimizer_param={'lr':0.001},   # SGDの学習率\n","                  evaluate_sample_num_per_epoch=1000,   # 各エポックで評価するサンプル数\n","                  verbose=False)        # 詳細な出力を表示しない\n","\n","# 学習を開始する。\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["今回のモデルの仕様では目標の精度には達していませんね。\n","\n","それぞれの層がどのような働きをしているか、インポートするライブラリを増やして、より高性能のモデルを考えてみると\n","\n","精度が向上するヒントが見つかるかもしれません。"],"metadata":{"id":"tVPHCHx1e3H5"}},{"cell_type":"markdown","metadata":{"id":"Hs_p7w3NpSR1"},"source":["## （参考）提出用データの作成の仕方\n","\n","上記のDeepConvNetでテスト用データを予測して提出用ファイルを出力するまでを\n","掲載してみました。\n","\n","このcsvファイルを「修了課題提出用サイト」にアップロードすると結果を確認することができます。"]},{"cell_type":"code","metadata":{"id":"QWtwP4xCMfYE"},"source":["import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding\n","\n","# テスト用のデータ（正解ラベルなし）のダウンロード\n","!wget 'https://drive.google.com/uc?export=download&id=18h1BsXvC6q_yZsO_TcONyotw5wXEU9b8' -O mnist_x_test.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2YCXoptcja4G"},"source":["import pandas as pd\n","import cupy as np\n","x_test = pd.read_csv('mnist_x_test.csv', index_col=0)\n","x_test = x_test.values\n","x_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YL7zSIkZh5_h"},"source":["# データの前処理\n","\n","# 画像データを正規化する（ピクセル値を0から1の範囲に変換）\n","# MNISTの画像のピクセル値は0から255の範囲のため、255で割ります。\n","x_test = x_test / 255.0\n","\n","# リスト形式のデータをNumPy配列に変換する。\n","x_test = np.array(x_test)\n","\n","# 画像データを格納するための空のリストを作成する。\n","train_images = []\n","\n","# 各画像データに対して処理を行う。\n","for i in range(x_test.shape[0]):\n","    # 画像データを28x28の2次元配列に変換し、np.expand_dimsを使用して次元を追加する。\n","    # これにより、画像は1x28x28の3次元配列にする。\n","    train_images.append(np.expand_dims(x_test[i].reshape(28, 28), axis=0))\n","\n","# リスト形式の画像データをNumPy配列に変換する。\n","x_test = np.array(train_images)\n","\n","# 最終的なデータの形状を出力します。\n","x_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VWMkqa-kHGT"},"source":["# データサイズが大きく、メモリがオーバーするためバッチで予測する\n","\n","# ミニバッチのサイズを設定する。\n","mini_batch = 100\n","\n","# データをミニバッチに分割して処理する。\n","for mini_x_test in range(1, len(x_test)//mini_batch+1):\n","\n","  # ミニバッチの開始と終了インデックスを計算する。\n","  start = (mini_x_test-1)*mini_batch\n","  end = mini_x_test*mini_batch\n","\n","  # ネットワークを使用して予測を行う。\n","  y_pred = network.predict(x_test[start:end])\n","\n","  # 最初のミニバッチの場合、予測結果をy_predsに格納する。\n","  # そうでない場合、以前の予測結果と新しい予測結果を結合する。\n","  if mini_x_test == 1:\n","    y_preds = y_pred\n","  else:\n","    y_preds = np.concatenate((y_preds, y_pred), axis=0)\n","\n","# 各予測の最大値のインデックス（クラスラベル）を取得する。\n","y_preds = np.argmax(y_preds, axis=1)\n","\n","# 最終的な予測結果の形状を出力する。\n","y_preds.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6CNNxveq2rb"},"source":["# csvファイルで出力する\n","pd.DataFrame(y_preds.get(), columns=['number']).to_csv('y_pred.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0xyB7YCZte2n"},"execution_count":null,"outputs":[]}]}