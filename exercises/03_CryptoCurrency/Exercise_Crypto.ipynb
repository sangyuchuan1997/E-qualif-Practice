{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64tFO6F51JiM"
   },
   "source": [
    "# 修了課題③ 仮想通貨\n",
    "\n",
    "**データセット**：仮想通貨の週単位のデータ  \n",
    "※１週間のうち６日間の取引価格の終値を学習データとして利用して、最後の１日の取引価格の終値を予測する。これは、１週間の取引価格の間に依存関係があり、正しく傾向を学習すれば、６日間の取引価格から残りの１日間の価格が予測できるはずという仮説に基づいたものとなる。\n",
    "\n",
    "**合格基準**：RMSE 50未満\n",
    "  \n",
    "\n",
    "※補足 ( 仮想通貨に関して)  \n",
    "仮想通貨は、株式と同じように個別銘柄（ビットコインやイーサリアムなど）が存在し、日々仮想通貨市場にて取引が行われている。仮想通貨市場は為替市場などと同じように取引されており、例えば代表的な仮想通貨であるビットコインなどであれば、１ビットコイン＝〇〇ドルのような形で日々刻々と価格が変動している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5D8p3QP78cO"
   },
   "source": [
    "# データのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "72_mpSUW6Xkm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-27 04:22:37--  https://drive.google.com/uc?export=download&id=1kUfPb8qikA8rdQ26iVUxpod2Qjw3ct_O\n",
      "正在解析主机 drive.google.com (drive.google.com)... 142.251.222.14\n",
      "正在连接 drive.google.com (drive.google.com)|142.251.222.14|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 303 See Other\n",
      "位置：https://drive.usercontent.google.com/download?id=1kUfPb8qikA8rdQ26iVUxpod2Qjw3ct_O&export=download [跟随至新的 URL]\n",
      "--2025-01-27 04:22:38--  https://drive.usercontent.google.com/download?id=1kUfPb8qikA8rdQ26iVUxpod2Qjw3ct_O&export=download\n",
      "正在解析主机 drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.42.193\n",
      "正在连接 drive.usercontent.google.com (drive.usercontent.google.com)|142.251.42.193|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：24244 (24K) [application/octet-stream]\n",
      "正在保存至: “crypto_train.csv”\n",
      "\n",
      "crypto_train.csv    100%[===================>]  23.68K  --.-KB/s  用时 0.05s     \n",
      "\n",
      "2025-01-27 04:22:41 (511 KB/s) - 已保存 “crypto_train.csv” [24244/24244])\n",
      "\n",
      "--2025-01-27 04:22:41--  https://drive.google.com/uc?export=download&id=1VhzCcjNSDxGRG86Za653zHHpjVCdSPD3\n",
      "正在解析主机 drive.google.com (drive.google.com)... 142.251.222.14\n",
      "正在连接 drive.google.com (drive.google.com)|142.251.222.14|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 303 See Other\n",
      "位置：https://drive.usercontent.google.com/download?id=1VhzCcjNSDxGRG86Za653zHHpjVCdSPD3&export=download [跟随至新的 URL]\n",
      "--2025-01-27 04:22:42--  https://drive.usercontent.google.com/download?id=1VhzCcjNSDxGRG86Za653zHHpjVCdSPD3&export=download\n",
      "正在解析主机 drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.42.193\n",
      "正在连接 drive.usercontent.google.com (drive.usercontent.google.com)|142.251.42.193|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：1706 (1.7K) [application/octet-stream]\n",
      "正在保存至: “crypto_test_x.csv”\n",
      "\n",
      "crypto_test_x.csv   100%[===================>]   1.67K  --.-KB/s  用时 0s        \n",
      "\n",
      "2025-01-27 04:22:45 (24.7 MB/s) - 已保存 “crypto_test_x.csv” [1706/1706])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 学習データのダウンロード\n",
    "!wget 'https://drive.google.com/uc?export=download&id=1kUfPb8qikA8rdQ26iVUxpod2Qjw3ct_O' -O crypto_train.csv\n",
    "\n",
    "# テストデータのダウンロード\n",
    "!wget 'https://drive.google.com/uc?export=download&id=1VhzCcjNSDxGRG86Za653zHHpjVCdSPD3' -O crypto_test_x.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsRla1RB8eE_"
   },
   "source": [
    "# データの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "9gGwA35H8d6b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "vccbxcBcZqNT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mon</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Sun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144.539993</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>115.910004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112.300003</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>113.566002</td>\n",
       "      <td>112.669998</td>\n",
       "      <td>117.199997</td>\n",
       "      <td>115.242996</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.980003</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>114.220001</td>\n",
       "      <td>118.760002</td>\n",
       "      <td>123.014999</td>\n",
       "      <td>123.498001</td>\n",
       "      <td>121.989998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>122.879997</td>\n",
       "      <td>123.889000</td>\n",
       "      <td>126.699997</td>\n",
       "      <td>133.199997</td>\n",
       "      <td>131.979996</td>\n",
       "      <td>133.479996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129.744995</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>132.300003</td>\n",
       "      <td>128.798996</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>129.300003</td>\n",
       "      <td>122.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>739.247986</td>\n",
       "      <td>751.346985</td>\n",
       "      <td>744.593994</td>\n",
       "      <td>740.289001</td>\n",
       "      <td>741.648987</td>\n",
       "      <td>735.382019</td>\n",
       "      <td>732.034973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>735.812988</td>\n",
       "      <td>735.604004</td>\n",
       "      <td>745.690979</td>\n",
       "      <td>756.773987</td>\n",
       "      <td>777.943970</td>\n",
       "      <td>771.155029</td>\n",
       "      <td>773.872009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>758.700012</td>\n",
       "      <td>764.223999</td>\n",
       "      <td>768.132019</td>\n",
       "      <td>770.809998</td>\n",
       "      <td>772.794006</td>\n",
       "      <td>774.650024</td>\n",
       "      <td>769.731018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>780.086975</td>\n",
       "      <td>780.556030</td>\n",
       "      <td>781.481018</td>\n",
       "      <td>778.088013</td>\n",
       "      <td>784.906982</td>\n",
       "      <td>790.828979</td>\n",
       "      <td>790.530029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>792.713989</td>\n",
       "      <td>800.875977</td>\n",
       "      <td>834.281006</td>\n",
       "      <td>864.539978</td>\n",
       "      <td>921.984009</td>\n",
       "      <td>898.822021</td>\n",
       "      <td>896.182983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mon         Tue         Wed         Thu         Fri         Sat  \\\n",
       "0    144.539993  139.000000  116.989998  105.209999   97.750000  112.500000   \n",
       "1    112.300003  111.500000  113.566002  112.669998  117.199997  115.242996   \n",
       "2    117.980003  111.500000  114.220001  118.760002  123.014999  123.498001   \n",
       "3    122.000000  122.879997  123.889000  126.699997  133.199997  131.979996   \n",
       "4    129.744995  129.000000  132.300003  128.798996  129.000000  129.300003   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "186  739.247986  751.346985  744.593994  740.289001  741.648987  735.382019   \n",
       "187  735.812988  735.604004  745.690979  756.773987  777.943970  771.155029   \n",
       "188  758.700012  764.223999  768.132019  770.809998  772.794006  774.650024   \n",
       "189  780.086975  780.556030  781.481018  778.088013  784.906982  790.828979   \n",
       "190  792.713989  800.875977  834.281006  864.539978  921.984009  898.822021   \n",
       "\n",
       "            Sun  \n",
       "0    115.910004  \n",
       "1    115.000000  \n",
       "2    121.989998  \n",
       "3    133.479996  \n",
       "4    122.292000  \n",
       "..          ...  \n",
       "186  732.034973  \n",
       "187  773.872009  \n",
       "188  769.731018  \n",
       "189  790.530029  \n",
       "190  896.182983  \n",
       "\n",
       "[191 rows x 7 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習データの確認\n",
    "train = pd.read_csv('crypto_train.csv', index_col=0)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "G0HJnEN9eGST"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mon</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Sat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>902.200989</td>\n",
       "      <td>908.585022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>902.828003</td>\n",
       "      <td>907.679016</td>\n",
       "      <td>777.757019</td>\n",
       "      <td>804.833984</td>\n",
       "      <td>823.984009</td>\n",
       "      <td>818.411987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>831.533997</td>\n",
       "      <td>907.937988</td>\n",
       "      <td>886.617981</td>\n",
       "      <td>899.072998</td>\n",
       "      <td>895.026001</td>\n",
       "      <td>921.789001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>921.012024</td>\n",
       "      <td>892.687012</td>\n",
       "      <td>901.541992</td>\n",
       "      <td>917.585999</td>\n",
       "      <td>919.750000</td>\n",
       "      <td>921.590027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>920.382019</td>\n",
       "      <td>970.403015</td>\n",
       "      <td>989.023010</td>\n",
       "      <td>1011.799988</td>\n",
       "      <td>1029.910034</td>\n",
       "      <td>1042.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1038.150024</td>\n",
       "      <td>1061.349976</td>\n",
       "      <td>1063.069946</td>\n",
       "      <td>994.382996</td>\n",
       "      <td>988.674011</td>\n",
       "      <td>1004.450012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>990.642029</td>\n",
       "      <td>1004.549988</td>\n",
       "      <td>1007.479980</td>\n",
       "      <td>1027.439941</td>\n",
       "      <td>1046.209961</td>\n",
       "      <td>1054.420044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1079.979980</td>\n",
       "      <td>1115.300049</td>\n",
       "      <td>1117.439941</td>\n",
       "      <td>1166.719971</td>\n",
       "      <td>1173.680054</td>\n",
       "      <td>1143.839966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1179.969971</td>\n",
       "      <td>1179.969971</td>\n",
       "      <td>1222.500000</td>\n",
       "      <td>1251.010010</td>\n",
       "      <td>1274.989990</td>\n",
       "      <td>1255.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1272.829956</td>\n",
       "      <td>1223.540039</td>\n",
       "      <td>1150.000000</td>\n",
       "      <td>1188.489990</td>\n",
       "      <td>1116.719971</td>\n",
       "      <td>1175.829956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1231.920044</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>1249.609985</td>\n",
       "      <td>1187.810059</td>\n",
       "      <td>1100.229980</td>\n",
       "      <td>973.817993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1054.229980</td>\n",
       "      <td>1120.540039</td>\n",
       "      <td>1049.140015</td>\n",
       "      <td>1038.589966</td>\n",
       "      <td>937.520020</td>\n",
       "      <td>972.778992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1045.770020</td>\n",
       "      <td>1047.150024</td>\n",
       "      <td>1039.969971</td>\n",
       "      <td>1026.430054</td>\n",
       "      <td>1071.790039</td>\n",
       "      <td>1080.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1143.810059</td>\n",
       "      <td>1133.250000</td>\n",
       "      <td>1124.780029</td>\n",
       "      <td>1182.680054</td>\n",
       "      <td>1176.900024</td>\n",
       "      <td>1175.949951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1187.130005</td>\n",
       "      <td>1205.010010</td>\n",
       "      <td>1200.369995</td>\n",
       "      <td>1169.280029</td>\n",
       "      <td>1167.540039</td>\n",
       "      <td>1172.520020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1193.910034</td>\n",
       "      <td>1211.670044</td>\n",
       "      <td>1210.290039</td>\n",
       "      <td>1229.079956</td>\n",
       "      <td>1222.050049</td>\n",
       "      <td>1231.709961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mon          Tue          Wed          Thu          Fri  \\\n",
       "0   1021.750000  1043.839966  1154.729980  1013.380005   902.200989   \n",
       "1    902.828003   907.679016   777.757019   804.833984   823.984009   \n",
       "2    831.533997   907.937988   886.617981   899.072998   895.026001   \n",
       "3    921.012024   892.687012   901.541992   917.585999   919.750000   \n",
       "4    920.382019   970.403015   989.023010  1011.799988  1029.910034   \n",
       "5   1038.150024  1061.349976  1063.069946   994.382996   988.674011   \n",
       "6    990.642029  1004.549988  1007.479980  1027.439941  1046.209961   \n",
       "7   1079.979980  1115.300049  1117.439941  1166.719971  1173.680054   \n",
       "8   1179.969971  1179.969971  1222.500000  1251.010010  1274.989990   \n",
       "9   1272.829956  1223.540039  1150.000000  1188.489990  1116.719971   \n",
       "10  1231.920044  1240.000000  1249.609985  1187.810059  1100.229980   \n",
       "11  1054.229980  1120.540039  1049.140015  1038.589966   937.520020   \n",
       "12  1045.770020  1047.150024  1039.969971  1026.430054  1071.790039   \n",
       "13  1143.810059  1133.250000  1124.780029  1182.680054  1176.900024   \n",
       "14  1187.130005  1205.010010  1200.369995  1169.280029  1167.540039   \n",
       "15  1193.910034  1211.670044  1210.290039  1229.079956  1222.050049   \n",
       "\n",
       "            Sat  \n",
       "0    908.585022  \n",
       "1    818.411987  \n",
       "2    921.789001  \n",
       "3    921.590027  \n",
       "4   1042.900024  \n",
       "5   1004.450012  \n",
       "6   1054.420044  \n",
       "7   1143.839966  \n",
       "8   1255.150024  \n",
       "9   1175.829956  \n",
       "10   973.817993  \n",
       "11   972.778992  \n",
       "12  1080.500000  \n",
       "13  1175.949951  \n",
       "14  1172.520020  \n",
       "15  1231.709961  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストデータの確認\n",
    "test = pd.read_csv('crypto_test_x.csv', index_col=0)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9hyms5m-9CD"
   },
   "source": [
    "# モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangyuchuan/codes/E-qualif-Practice/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 正規化処理\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(train.values)\n",
    "\n",
    "# 正規化する\n",
    "s_train = scaler.transform(train)\n",
    "# 元の値に戻す場合\n",
    "inv_train = scaler.inverse_transform(s_train)\n",
    "\n",
    "# シーケンスデータの教師ありデータを作成する\n",
    "def make_seq(data, seq_len=6):\n",
    "    X = data[:, 0:seq_len]\n",
    "    X = np.expand_dims(X, axis=2)\n",
    "    Y = data[:, seq_len:]\n",
    "    return X, Y\n",
    "\n",
    "X, Y = make_seq(s_train)\n",
    "\n",
    "def train_val_split(X, Y, val_rate):\n",
    "  rate = int(X.shape[0]*(1-val_rate))\n",
    "  train_X, train_Y, val_X, val_Y = X[:rate], Y[:rate], X[rate:], Y[rate:]\n",
    "  return train_X, train_Y, val_X, val_Y\n",
    "\n",
    "train_X, train_Y, val_X, val_Y = train_val_split(X, Y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((171, 6, 1), (171, 1), (20, 6, 1), (20, 1))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_Y.shape, val_X.shape, val_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "r_Ba6RpeW254"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM モデル\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size_1=64, hidden_size_2=32, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size_1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(hidden_size_1, hidden_size_2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_size_2, output_size)\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        # self.modelをselfに修正\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First LSTM layer\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout2(out[:, -1, :])  # Get last output\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():  # 構文を修正\n",
    "            return self.forward(x)\n",
    "    \n",
    "    def train_model(self, train_x, train_y, val_x, val_y, epochs):\n",
    "        \"\"\"モデルの学習を行う\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.forward(train_x)\n",
    "            loss = self.criterion(outputs, train_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Validation phase\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self.forward(val_x)\n",
    "                val_loss = self.criterion(val_outputs, val_y)\n",
    "            \n",
    "            # Loss values\n",
    "            train_losses.append(loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        \n",
    "        return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Train Loss: 0.1361, Val Loss: 0.3516\n",
      "Epoch [20/5000], Train Loss: 0.0510, Val Loss: 0.0497\n",
      "Epoch [30/5000], Train Loss: 0.0461, Val Loss: 0.0792\n",
      "Epoch [40/5000], Train Loss: 0.0404, Val Loss: 0.1072\n",
      "Epoch [50/5000], Train Loss: 0.0354, Val Loss: 0.0459\n",
      "Epoch [60/5000], Train Loss: 0.0281, Val Loss: 0.0433\n",
      "Epoch [70/5000], Train Loss: 0.0125, Val Loss: 0.0089\n",
      "Epoch [80/5000], Train Loss: 0.0091, Val Loss: 0.0029\n",
      "Epoch [90/5000], Train Loss: 0.0087, Val Loss: 0.0016\n",
      "Epoch [100/5000], Train Loss: 0.0078, Val Loss: 0.0005\n",
      "Epoch [110/5000], Train Loss: 0.0047, Val Loss: 0.0007\n",
      "Epoch [120/5000], Train Loss: 0.0067, Val Loss: 0.0014\n",
      "Epoch [130/5000], Train Loss: 0.0064, Val Loss: 0.0013\n",
      "Epoch [140/5000], Train Loss: 0.0054, Val Loss: 0.0005\n",
      "Epoch [150/5000], Train Loss: 0.0045, Val Loss: 0.0005\n",
      "Epoch [160/5000], Train Loss: 0.0045, Val Loss: 0.0004\n",
      "Epoch [170/5000], Train Loss: 0.0056, Val Loss: 0.0005\n",
      "Epoch [180/5000], Train Loss: 0.0036, Val Loss: 0.0006\n",
      "Epoch [190/5000], Train Loss: 0.0048, Val Loss: 0.0006\n",
      "Epoch [200/5000], Train Loss: 0.0048, Val Loss: 0.0006\n",
      "Epoch [210/5000], Train Loss: 0.0038, Val Loss: 0.0013\n",
      "Epoch [220/5000], Train Loss: 0.0040, Val Loss: 0.0014\n",
      "Epoch [230/5000], Train Loss: 0.0043, Val Loss: 0.0008\n",
      "Epoch [240/5000], Train Loss: 0.0038, Val Loss: 0.0007\n",
      "Epoch [250/5000], Train Loss: 0.0055, Val Loss: 0.0011\n",
      "Epoch [260/5000], Train Loss: 0.0048, Val Loss: 0.0012\n",
      "Epoch [270/5000], Train Loss: 0.0046, Val Loss: 0.0009\n",
      "Epoch [280/5000], Train Loss: 0.0039, Val Loss: 0.0011\n",
      "Epoch [290/5000], Train Loss: 0.0037, Val Loss: 0.0006\n",
      "Epoch [300/5000], Train Loss: 0.0043, Val Loss: 0.0006\n",
      "Epoch [310/5000], Train Loss: 0.0036, Val Loss: 0.0011\n",
      "Epoch [320/5000], Train Loss: 0.0052, Val Loss: 0.0010\n",
      "Epoch [330/5000], Train Loss: 0.0034, Val Loss: 0.0006\n",
      "Epoch [340/5000], Train Loss: 0.0054, Val Loss: 0.0006\n",
      "Epoch [350/5000], Train Loss: 0.0037, Val Loss: 0.0008\n",
      "Epoch [360/5000], Train Loss: 0.0041, Val Loss: 0.0007\n",
      "Epoch [370/5000], Train Loss: 0.0041, Val Loss: 0.0006\n",
      "Epoch [380/5000], Train Loss: 0.0036, Val Loss: 0.0007\n",
      "Epoch [390/5000], Train Loss: 0.0032, Val Loss: 0.0011\n",
      "Epoch [400/5000], Train Loss: 0.0043, Val Loss: 0.0008\n",
      "Epoch [410/5000], Train Loss: 0.0039, Val Loss: 0.0011\n",
      "Epoch [420/5000], Train Loss: 0.0032, Val Loss: 0.0011\n",
      "Epoch [430/5000], Train Loss: 0.0035, Val Loss: 0.0008\n",
      "Epoch [440/5000], Train Loss: 0.0034, Val Loss: 0.0010\n",
      "Epoch [450/5000], Train Loss: 0.0032, Val Loss: 0.0013\n",
      "Epoch [460/5000], Train Loss: 0.0030, Val Loss: 0.0007\n",
      "Epoch [470/5000], Train Loss: 0.0035, Val Loss: 0.0007\n",
      "Epoch [480/5000], Train Loss: 0.0034, Val Loss: 0.0006\n",
      "Epoch [490/5000], Train Loss: 0.0037, Val Loss: 0.0006\n",
      "Epoch [500/5000], Train Loss: 0.0040, Val Loss: 0.0006\n",
      "Epoch [510/5000], Train Loss: 0.0048, Val Loss: 0.0007\n",
      "Epoch [520/5000], Train Loss: 0.0029, Val Loss: 0.0007\n",
      "Epoch [530/5000], Train Loss: 0.0031, Val Loss: 0.0005\n",
      "Epoch [540/5000], Train Loss: 0.0035, Val Loss: 0.0008\n",
      "Epoch [550/5000], Train Loss: 0.0041, Val Loss: 0.0006\n",
      "Epoch [560/5000], Train Loss: 0.0028, Val Loss: 0.0006\n",
      "Epoch [570/5000], Train Loss: 0.0033, Val Loss: 0.0006\n",
      "Epoch [580/5000], Train Loss: 0.0029, Val Loss: 0.0006\n",
      "Epoch [590/5000], Train Loss: 0.0036, Val Loss: 0.0005\n",
      "Epoch [600/5000], Train Loss: 0.0030, Val Loss: 0.0007\n",
      "Epoch [610/5000], Train Loss: 0.0035, Val Loss: 0.0006\n",
      "Epoch [620/5000], Train Loss: 0.0032, Val Loss: 0.0006\n",
      "Epoch [630/5000], Train Loss: 0.0036, Val Loss: 0.0006\n",
      "Epoch [640/5000], Train Loss: 0.0034, Val Loss: 0.0005\n",
      "Epoch [650/5000], Train Loss: 0.0038, Val Loss: 0.0005\n",
      "Epoch [660/5000], Train Loss: 0.0039, Val Loss: 0.0005\n",
      "Epoch [670/5000], Train Loss: 0.0033, Val Loss: 0.0008\n",
      "Epoch [680/5000], Train Loss: 0.0036, Val Loss: 0.0008\n",
      "Epoch [690/5000], Train Loss: 0.0027, Val Loss: 0.0006\n",
      "Epoch [700/5000], Train Loss: 0.0045, Val Loss: 0.0005\n",
      "Epoch [710/5000], Train Loss: 0.0028, Val Loss: 0.0006\n",
      "Epoch [720/5000], Train Loss: 0.0036, Val Loss: 0.0005\n",
      "Epoch [730/5000], Train Loss: 0.0028, Val Loss: 0.0005\n",
      "Epoch [740/5000], Train Loss: 0.0029, Val Loss: 0.0005\n",
      "Epoch [750/5000], Train Loss: 0.0035, Val Loss: 0.0006\n",
      "Epoch [760/5000], Train Loss: 0.0027, Val Loss: 0.0006\n",
      "Epoch [770/5000], Train Loss: 0.0034, Val Loss: 0.0009\n",
      "Epoch [780/5000], Train Loss: 0.0029, Val Loss: 0.0012\n",
      "Epoch [790/5000], Train Loss: 0.0036, Val Loss: 0.0005\n",
      "Epoch [800/5000], Train Loss: 0.0034, Val Loss: 0.0006\n",
      "Epoch [810/5000], Train Loss: 0.0035, Val Loss: 0.0013\n",
      "Epoch [820/5000], Train Loss: 0.0034, Val Loss: 0.0006\n",
      "Epoch [830/5000], Train Loss: 0.0031, Val Loss: 0.0005\n",
      "Epoch [840/5000], Train Loss: 0.0038, Val Loss: 0.0004\n",
      "Epoch [850/5000], Train Loss: 0.0040, Val Loss: 0.0005\n",
      "Epoch [860/5000], Train Loss: 0.0035, Val Loss: 0.0004\n",
      "Epoch [870/5000], Train Loss: 0.0030, Val Loss: 0.0005\n",
      "Epoch [880/5000], Train Loss: 0.0031, Val Loss: 0.0004\n",
      "Epoch [890/5000], Train Loss: 0.0035, Val Loss: 0.0005\n",
      "Epoch [900/5000], Train Loss: 0.0040, Val Loss: 0.0004\n",
      "Epoch [910/5000], Train Loss: 0.0025, Val Loss: 0.0006\n",
      "Epoch [920/5000], Train Loss: 0.0026, Val Loss: 0.0004\n",
      "Epoch [930/5000], Train Loss: 0.0026, Val Loss: 0.0007\n",
      "Epoch [940/5000], Train Loss: 0.0026, Val Loss: 0.0004\n",
      "Epoch [950/5000], Train Loss: 0.0032, Val Loss: 0.0004\n",
      "Epoch [960/5000], Train Loss: 0.0028, Val Loss: 0.0010\n",
      "Epoch [970/5000], Train Loss: 0.0029, Val Loss: 0.0003\n",
      "Epoch [980/5000], Train Loss: 0.0034, Val Loss: 0.0004\n",
      "Epoch [990/5000], Train Loss: 0.0031, Val Loss: 0.0013\n",
      "Epoch [1000/5000], Train Loss: 0.0032, Val Loss: 0.0009\n",
      "Epoch [1010/5000], Train Loss: 0.0029, Val Loss: 0.0005\n",
      "Epoch [1020/5000], Train Loss: 0.0033, Val Loss: 0.0006\n",
      "Epoch [1030/5000], Train Loss: 0.0026, Val Loss: 0.0005\n",
      "Epoch [1040/5000], Train Loss: 0.0024, Val Loss: 0.0015\n",
      "Epoch [1050/5000], Train Loss: 0.0024, Val Loss: 0.0012\n",
      "Epoch [1060/5000], Train Loss: 0.0029, Val Loss: 0.0007\n",
      "Epoch [1070/5000], Train Loss: 0.0027, Val Loss: 0.0003\n",
      "Epoch [1080/5000], Train Loss: 0.0023, Val Loss: 0.0006\n",
      "Epoch [1090/5000], Train Loss: 0.0033, Val Loss: 0.0014\n",
      "Epoch [1100/5000], Train Loss: 0.0033, Val Loss: 0.0005\n",
      "Epoch [1110/5000], Train Loss: 0.0029, Val Loss: 0.0004\n",
      "Epoch [1120/5000], Train Loss: 0.0022, Val Loss: 0.0004\n",
      "Epoch [1130/5000], Train Loss: 0.0030, Val Loss: 0.0009\n",
      "Epoch [1140/5000], Train Loss: 0.0025, Val Loss: 0.0007\n",
      "Epoch [1150/5000], Train Loss: 0.0022, Val Loss: 0.0003\n",
      "Epoch [1160/5000], Train Loss: 0.0023, Val Loss: 0.0006\n",
      "Epoch [1170/5000], Train Loss: 0.0022, Val Loss: 0.0003\n",
      "Epoch [1180/5000], Train Loss: 0.0025, Val Loss: 0.0006\n",
      "Epoch [1190/5000], Train Loss: 0.0030, Val Loss: 0.0005\n",
      "Epoch [1200/5000], Train Loss: 0.0029, Val Loss: 0.0007\n",
      "Epoch [1210/5000], Train Loss: 0.0020, Val Loss: 0.0003\n",
      "Epoch [1220/5000], Train Loss: 0.0026, Val Loss: 0.0004\n",
      "Epoch [1230/5000], Train Loss: 0.0032, Val Loss: 0.0004\n",
      "Epoch [1240/5000], Train Loss: 0.0025, Val Loss: 0.0004\n",
      "Epoch [1250/5000], Train Loss: 0.0023, Val Loss: 0.0003\n",
      "Epoch [1260/5000], Train Loss: 0.0028, Val Loss: 0.0003\n",
      "Epoch [1270/5000], Train Loss: 0.0024, Val Loss: 0.0003\n",
      "Epoch [1280/5000], Train Loss: 0.0037, Val Loss: 0.0003\n",
      "Epoch [1290/5000], Train Loss: 0.0023, Val Loss: 0.0005\n",
      "Epoch [1300/5000], Train Loss: 0.0024, Val Loss: 0.0004\n",
      "Epoch [1310/5000], Train Loss: 0.0023, Val Loss: 0.0003\n",
      "Epoch [1320/5000], Train Loss: 0.0021, Val Loss: 0.0003\n",
      "Epoch [1330/5000], Train Loss: 0.0019, Val Loss: 0.0003\n",
      "Epoch [1340/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [1350/5000], Train Loss: 0.0032, Val Loss: 0.0002\n",
      "Epoch [1360/5000], Train Loss: 0.0023, Val Loss: 0.0004\n",
      "Epoch [1370/5000], Train Loss: 0.0020, Val Loss: 0.0003\n",
      "Epoch [1380/5000], Train Loss: 0.0023, Val Loss: 0.0002\n",
      "Epoch [1390/5000], Train Loss: 0.0022, Val Loss: 0.0002\n",
      "Epoch [1400/5000], Train Loss: 0.0025, Val Loss: 0.0004\n",
      "Epoch [1410/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [1420/5000], Train Loss: 0.0027, Val Loss: 0.0008\n",
      "Epoch [1430/5000], Train Loss: 0.0031, Val Loss: 0.0004\n",
      "Epoch [1440/5000], Train Loss: 0.0023, Val Loss: 0.0007\n",
      "Epoch [1450/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [1460/5000], Train Loss: 0.0016, Val Loss: 0.0003\n",
      "Epoch [1470/5000], Train Loss: 0.0024, Val Loss: 0.0003\n",
      "Epoch [1480/5000], Train Loss: 0.0023, Val Loss: 0.0003\n",
      "Epoch [1490/5000], Train Loss: 0.0020, Val Loss: 0.0002\n",
      "Epoch [1500/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [1510/5000], Train Loss: 0.0018, Val Loss: 0.0003\n",
      "Epoch [1520/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [1530/5000], Train Loss: 0.0020, Val Loss: 0.0002\n",
      "Epoch [1540/5000], Train Loss: 0.0021, Val Loss: 0.0005\n",
      "Epoch [1550/5000], Train Loss: 0.0021, Val Loss: 0.0005\n",
      "Epoch [1560/5000], Train Loss: 0.0024, Val Loss: 0.0002\n",
      "Epoch [1570/5000], Train Loss: 0.0027, Val Loss: 0.0006\n",
      "Epoch [1580/5000], Train Loss: 0.0021, Val Loss: 0.0003\n",
      "Epoch [1590/5000], Train Loss: 0.0019, Val Loss: 0.0004\n",
      "Epoch [1600/5000], Train Loss: 0.0029, Val Loss: 0.0002\n",
      "Epoch [1610/5000], Train Loss: 0.0032, Val Loss: 0.0004\n",
      "Epoch [1620/5000], Train Loss: 0.0025, Val Loss: 0.0002\n",
      "Epoch [1630/5000], Train Loss: 0.0022, Val Loss: 0.0008\n",
      "Epoch [1640/5000], Train Loss: 0.0020, Val Loss: 0.0003\n",
      "Epoch [1650/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [1660/5000], Train Loss: 0.0018, Val Loss: 0.0005\n",
      "Epoch [1670/5000], Train Loss: 0.0028, Val Loss: 0.0005\n",
      "Epoch [1680/5000], Train Loss: 0.0020, Val Loss: 0.0004\n",
      "Epoch [1690/5000], Train Loss: 0.0021, Val Loss: 0.0003\n",
      "Epoch [1700/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [1710/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [1720/5000], Train Loss: 0.0026, Val Loss: 0.0003\n",
      "Epoch [1730/5000], Train Loss: 0.0024, Val Loss: 0.0002\n",
      "Epoch [1740/5000], Train Loss: 0.0022, Val Loss: 0.0003\n",
      "Epoch [1750/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [1760/5000], Train Loss: 0.0017, Val Loss: 0.0003\n",
      "Epoch [1770/5000], Train Loss: 0.0025, Val Loss: 0.0006\n",
      "Epoch [1780/5000], Train Loss: 0.0027, Val Loss: 0.0002\n",
      "Epoch [1790/5000], Train Loss: 0.0022, Val Loss: 0.0005\n",
      "Epoch [1800/5000], Train Loss: 0.0023, Val Loss: 0.0002\n",
      "Epoch [1810/5000], Train Loss: 0.0025, Val Loss: 0.0002\n",
      "Epoch [1820/5000], Train Loss: 0.0028, Val Loss: 0.0003\n",
      "Epoch [1830/5000], Train Loss: 0.0026, Val Loss: 0.0003\n",
      "Epoch [1840/5000], Train Loss: 0.0024, Val Loss: 0.0002\n",
      "Epoch [1850/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [1860/5000], Train Loss: 0.0021, Val Loss: 0.0003\n",
      "Epoch [1870/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [1880/5000], Train Loss: 0.0020, Val Loss: 0.0004\n",
      "Epoch [1890/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [1900/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [1910/5000], Train Loss: 0.0027, Val Loss: 0.0003\n",
      "Epoch [1920/5000], Train Loss: 0.0017, Val Loss: 0.0003\n",
      "Epoch [1930/5000], Train Loss: 0.0023, Val Loss: 0.0004\n",
      "Epoch [1940/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [1950/5000], Train Loss: 0.0020, Val Loss: 0.0005\n",
      "Epoch [1960/5000], Train Loss: 0.0017, Val Loss: 0.0004\n",
      "Epoch [1970/5000], Train Loss: 0.0023, Val Loss: 0.0005\n",
      "Epoch [1980/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [1990/5000], Train Loss: 0.0022, Val Loss: 0.0005\n",
      "Epoch [2000/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2010/5000], Train Loss: 0.0014, Val Loss: 0.0005\n",
      "Epoch [2020/5000], Train Loss: 0.0020, Val Loss: 0.0002\n",
      "Epoch [2030/5000], Train Loss: 0.0020, Val Loss: 0.0002\n",
      "Epoch [2040/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [2050/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [2060/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [2070/5000], Train Loss: 0.0017, Val Loss: 0.0002\n",
      "Epoch [2080/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [2090/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [2100/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [2110/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2120/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [2130/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [2140/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2150/5000], Train Loss: 0.0026, Val Loss: 0.0002\n",
      "Epoch [2160/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2170/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2180/5000], Train Loss: 0.0022, Val Loss: 0.0002\n",
      "Epoch [2190/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2200/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [2210/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [2220/5000], Train Loss: 0.0017, Val Loss: 0.0002\n",
      "Epoch [2230/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2240/5000], Train Loss: 0.0011, Val Loss: 0.0003\n",
      "Epoch [2250/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2260/5000], Train Loss: 0.0021, Val Loss: 0.0002\n",
      "Epoch [2270/5000], Train Loss: 0.0016, Val Loss: 0.0003\n",
      "Epoch [2280/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2290/5000], Train Loss: 0.0014, Val Loss: 0.0004\n",
      "Epoch [2300/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2310/5000], Train Loss: 0.0017, Val Loss: 0.0003\n",
      "Epoch [2320/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [2330/5000], Train Loss: 0.0016, Val Loss: 0.0007\n",
      "Epoch [2340/5000], Train Loss: 0.0020, Val Loss: 0.0002\n",
      "Epoch [2350/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [2360/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2370/5000], Train Loss: 0.0017, Val Loss: 0.0002\n",
      "Epoch [2380/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [2390/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2400/5000], Train Loss: 0.0015, Val Loss: 0.0003\n",
      "Epoch [2410/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2420/5000], Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch [2430/5000], Train Loss: 0.0018, Val Loss: 0.0003\n",
      "Epoch [2440/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2450/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2460/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2470/5000], Train Loss: 0.0017, Val Loss: 0.0005\n",
      "Epoch [2480/5000], Train Loss: 0.0020, Val Loss: 0.0002\n",
      "Epoch [2490/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2500/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2510/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2520/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [2530/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2540/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2550/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2560/5000], Train Loss: 0.0019, Val Loss: 0.0004\n",
      "Epoch [2570/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2580/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [2590/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2600/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [2610/5000], Train Loss: 0.0014, Val Loss: 0.0003\n",
      "Epoch [2620/5000], Train Loss: 0.0018, Val Loss: 0.0005\n",
      "Epoch [2630/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2640/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2650/5000], Train Loss: 0.0016, Val Loss: 0.0003\n",
      "Epoch [2660/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2670/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [2680/5000], Train Loss: 0.0015, Val Loss: 0.0003\n",
      "Epoch [2690/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [2700/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2710/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [2720/5000], Train Loss: 0.0015, Val Loss: 0.0007\n",
      "Epoch [2730/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2740/5000], Train Loss: 0.0015, Val Loss: 0.0003\n",
      "Epoch [2750/5000], Train Loss: 0.0013, Val Loss: 0.0004\n",
      "Epoch [2760/5000], Train Loss: 0.0014, Val Loss: 0.0004\n",
      "Epoch [2770/5000], Train Loss: 0.0014, Val Loss: 0.0003\n",
      "Epoch [2780/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [2790/5000], Train Loss: 0.0012, Val Loss: 0.0004\n",
      "Epoch [2800/5000], Train Loss: 0.0013, Val Loss: 0.0003\n",
      "Epoch [2810/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2820/5000], Train Loss: 0.0012, Val Loss: 0.0003\n",
      "Epoch [2830/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [2840/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2850/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2860/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [2870/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [2880/5000], Train Loss: 0.0018, Val Loss: 0.0002\n",
      "Epoch [2890/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [2900/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [2910/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2920/5000], Train Loss: 0.0016, Val Loss: 0.0002\n",
      "Epoch [2930/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2940/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2950/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [2960/5000], Train Loss: 0.0015, Val Loss: 0.0002\n",
      "Epoch [2970/5000], Train Loss: 0.0015, Val Loss: 0.0003\n",
      "Epoch [2980/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [2990/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3000/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3010/5000], Train Loss: 0.0011, Val Loss: 0.0003\n",
      "Epoch [3020/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [3030/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3040/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3050/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3060/5000], Train Loss: 0.0012, Val Loss: 0.0003\n",
      "Epoch [3070/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3080/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3090/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3100/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3110/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [3120/5000], Train Loss: 0.0015, Val Loss: 0.0003\n",
      "Epoch [3130/5000], Train Loss: 0.0011, Val Loss: 0.0003\n",
      "Epoch [3140/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3150/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3160/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3170/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3180/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3190/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3200/5000], Train Loss: 0.0013, Val Loss: 0.0003\n",
      "Epoch [3210/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3220/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3230/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3240/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3250/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3260/5000], Train Loss: 0.0011, Val Loss: 0.0003\n",
      "Epoch [3270/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3280/5000], Train Loss: 0.0014, Val Loss: 0.0003\n",
      "Epoch [3290/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3300/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3310/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [3320/5000], Train Loss: 0.0012, Val Loss: 0.0003\n",
      "Epoch [3330/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [3340/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3350/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [3360/5000], Train Loss: 0.0013, Val Loss: 0.0005\n",
      "Epoch [3370/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3380/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3390/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3400/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3410/5000], Train Loss: 0.0014, Val Loss: 0.0003\n",
      "Epoch [3420/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3430/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3440/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3450/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [3460/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3470/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [3480/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3490/5000], Train Loss: 0.0009, Val Loss: 0.0004\n",
      "Epoch [3500/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3510/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3520/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [3530/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3540/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3550/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3560/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3570/5000], Train Loss: 0.0010, Val Loss: 0.0004\n",
      "Epoch [3580/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3590/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3600/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3610/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3620/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [3630/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3640/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [3650/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [3660/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3670/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [3680/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3690/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3700/5000], Train Loss: 0.0008, Val Loss: 0.0004\n",
      "Epoch [3710/5000], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [3720/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3730/5000], Train Loss: 0.0008, Val Loss: 0.0004\n",
      "Epoch [3740/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3750/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3760/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [3770/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3780/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3790/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3800/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3810/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [3820/5000], Train Loss: 0.0013, Val Loss: 0.0002\n",
      "Epoch [3830/5000], Train Loss: 0.0013, Val Loss: 0.0004\n",
      "Epoch [3840/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3850/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3860/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3870/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3880/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3890/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3900/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [3910/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3920/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [3930/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [3940/5000], Train Loss: 0.0007, Val Loss: 0.0006\n",
      "Epoch [3950/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [3960/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [3970/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [3980/5000], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [3990/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4000/5000], Train Loss: 0.0014, Val Loss: 0.0002\n",
      "Epoch [4010/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4020/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [4030/5000], Train Loss: 0.0010, Val Loss: 0.0004\n",
      "Epoch [4040/5000], Train Loss: 0.0006, Val Loss: 0.0003\n",
      "Epoch [4050/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4060/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4070/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4080/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4090/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4100/5000], Train Loss: 0.0011, Val Loss: 0.0004\n",
      "Epoch [4110/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [4120/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4130/5000], Train Loss: 0.0006, Val Loss: 0.0002\n",
      "Epoch [4140/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4150/5000], Train Loss: 0.0007, Val Loss: 0.0004\n",
      "Epoch [4160/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4170/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4180/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4190/5000], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [4200/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [4210/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4220/5000], Train Loss: 0.0011, Val Loss: 0.0002\n",
      "Epoch [4230/5000], Train Loss: 0.0009, Val Loss: 0.0005\n",
      "Epoch [4240/5000], Train Loss: 0.0016, Val Loss: 0.0003\n",
      "Epoch [4250/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4260/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4270/5000], Train Loss: 0.0006, Val Loss: 0.0002\n",
      "Epoch [4280/5000], Train Loss: 0.0010, Val Loss: 0.0005\n",
      "Epoch [4290/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4300/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4310/5000], Train Loss: 0.0011, Val Loss: 0.0003\n",
      "Epoch [4320/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4330/5000], Train Loss: 0.0006, Val Loss: 0.0003\n",
      "Epoch [4340/5000], Train Loss: 0.0008, Val Loss: 0.0004\n",
      "Epoch [4350/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [4360/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [4370/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4380/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [4390/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4400/5000], Train Loss: 0.0008, Val Loss: 0.0005\n",
      "Epoch [4410/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4420/5000], Train Loss: 0.0012, Val Loss: 0.0002\n",
      "Epoch [4430/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4440/5000], Train Loss: 0.0009, Val Loss: 0.0004\n",
      "Epoch [4450/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [4460/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4470/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [4480/5000], Train Loss: 0.0005, Val Loss: 0.0002\n",
      "Epoch [4490/5000], Train Loss: 0.0008, Val Loss: 0.0004\n",
      "Epoch [4500/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4510/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4520/5000], Train Loss: 0.0010, Val Loss: 0.0004\n",
      "Epoch [4530/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4540/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4550/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4560/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4570/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4580/5000], Train Loss: 0.0007, Val Loss: 0.0004\n",
      "Epoch [4590/5000], Train Loss: 0.0008, Val Loss: 0.0004\n",
      "Epoch [4600/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4610/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4620/5000], Train Loss: 0.0013, Val Loss: 0.0003\n",
      "Epoch [4630/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4640/5000], Train Loss: 0.0008, Val Loss: 0.0005\n",
      "Epoch [4650/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4660/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [4670/5000], Train Loss: 0.0012, Val Loss: 0.0005\n",
      "Epoch [4680/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4690/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4700/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [4710/5000], Train Loss: 0.0008, Val Loss: 0.0005\n",
      "Epoch [4720/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4730/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4740/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [4750/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4760/5000], Train Loss: 0.0006, Val Loss: 0.0002\n",
      "Epoch [4770/5000], Train Loss: 0.0007, Val Loss: 0.0004\n",
      "Epoch [4780/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [4790/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [4800/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4810/5000], Train Loss: 0.0006, Val Loss: 0.0002\n",
      "Epoch [4820/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4830/5000], Train Loss: 0.0008, Val Loss: 0.0003\n",
      "Epoch [4840/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [4850/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4860/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4870/5000], Train Loss: 0.0007, Val Loss: 0.0003\n",
      "Epoch [4880/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4890/5000], Train Loss: 0.0009, Val Loss: 0.0003\n",
      "Epoch [4900/5000], Train Loss: 0.0010, Val Loss: 0.0003\n",
      "Epoch [4910/5000], Train Loss: 0.0011, Val Loss: 0.0003\n",
      "Epoch [4920/5000], Train Loss: 0.0006, Val Loss: 0.0004\n",
      "Epoch [4930/5000], Train Loss: 0.0008, Val Loss: 0.0002\n",
      "Epoch [4940/5000], Train Loss: 0.0007, Val Loss: 0.0003\n",
      "Epoch [4950/5000], Train Loss: 0.0007, Val Loss: 0.0002\n",
      "Epoch [4960/5000], Train Loss: 0.0007, Val Loss: 0.0005\n",
      "Epoch [4970/5000], Train Loss: 0.0007, Val Loss: 0.0003\n",
      "Epoch [4980/5000], Train Loss: 0.0009, Val Loss: 0.0002\n",
      "Epoch [4990/5000], Train Loss: 0.0010, Val Loss: 0.0002\n",
      "Epoch [5000/5000], Train Loss: 0.0011, Val Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# モデルのインスタンス化\n",
    "model = LSTMModel(input_size=1, hidden_size_1=64, hidden_size_2=32, output_size=1)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "train_X = torch.FloatTensor(train_X)\n",
    "train_Y = torch.FloatTensor(train_Y)\n",
    "val_X = torch.FloatTensor(val_X)\n",
    "val_Y = torch.FloatTensor(val_Y)\n",
    "\n",
    "# モデルの学習\n",
    "train_losses, val_losses = model.train_model(train_X, train_Y, val_X, val_Y, epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yUeMQ9Q_AKz"
   },
   "source": [
    "# 提出形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最終予測結果の形状: (16,)\n",
      "\n",
      "予測値:\n",
      "[ 917.4505768   817.13749242  937.39630651  948.76724159 1102.65127572\n",
      " 1023.53768176 1110.48132536 1217.05832581 1258.03890898 1247.34642289\n",
      " 1092.50287642  972.64303589 1131.27090244 1240.19059209 1240.47824144\n",
      " 1264.04138303]\n"
     ]
    }
   ],
   "source": [
    "# 予測\n",
    "\n",
    "# スケーリング\n",
    "test_x = scaler.fit_transform(test.values)\n",
    "# テストデータの形状を(16, 6, 1)に変更\n",
    "test_x = test_x.reshape(test_x.shape[0], test_x.shape[1], 1)\n",
    "\n",
    "# PyTorchのテンソルに変換\n",
    "test_x = torch.FloatTensor(test_x)\n",
    "\n",
    "# 予測実行\n",
    "model.eval()  # モデルを評価モードに設定\n",
    "with torch.no_grad():\n",
    "    pred = model(test_x)\n",
    "\n",
    "# 予測結果をnumpy配列に変換\n",
    "pred = pred.numpy()\n",
    "\n",
    "# スケール変換のために新しい配列を作成\n",
    "scaled_pred = np.zeros((pred.shape[0], 6))  # (16, 6)の形状\n",
    "scaled_pred[:, -1] = pred.flatten()  # 最後の列に予測値を設定\n",
    "\n",
    "# スケーラーを使って予測値を元のスケールに戻す\n",
    "scaled_pred = scaler.inverse_transform(scaled_pred)\n",
    "\n",
    "# 最後の列（予測値）だけを取り出す\n",
    "final_predictions = scaled_pred[:, -1]\n",
    "\n",
    "# 結果の確認\n",
    "print(\"最終予測結果の形状:\", final_predictions.shape)\n",
    "print(\"\\n予測値:\")\n",
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "0IVmbl1n_C59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917.450577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>817.137492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>937.396307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>948.767242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1102.651276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1023.537682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1110.481325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1217.058326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1258.038909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1247.346423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1092.502876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>972.643036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1131.270902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1240.190592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1240.478241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1264.041383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sun\n",
       "0    917.450577\n",
       "1    817.137492\n",
       "2    937.396307\n",
       "3    948.767242\n",
       "4   1102.651276\n",
       "5   1023.537682\n",
       "6   1110.481325\n",
       "7   1217.058326\n",
       "8   1258.038909\n",
       "9   1247.346423\n",
       "10  1092.502876\n",
       "11   972.643036\n",
       "12  1131.270902\n",
       "13  1240.190592\n",
       "14  1240.478241\n",
       "15  1264.041383"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提出形式の確認\n",
    "pred = pd.DataFrame(final_predictions, columns=['Sun'])\n",
    "# 形状が（16, 1）になっていることを確認して下さい。\n",
    "print(pred.shape)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "_p6di0bL_YoR"
   },
   "outputs": [],
   "source": [
    "# csv形式での提出をお願いします。\n",
    "pred.to_csv('crypto_pred.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
